#!/usr/bin/env python2

import sys, time, os
sys.path.append("/home/hedeer/code/lib")
from collections import defaultdict
import operator
import cPickle as pickle

import numpy as np

import NN.fofe as fofe
from NN import layer, neural, trainer, dataset, cmdloop, transformer
from NN.cost import cost
from NN.activation import activation

BEGIN_SEQ = "__BEGIN_SEQ___"
END_SEQ = "__END_SEQ__"
UNK = "__UNKNOWN__"

class FOFENet():
    def __init__(self, sequence = [], forget_factor=0.65, word_cover = 0.95, embedding_dim = 200, backend = 'cudarray'):
        """
        type sequence: list of list
        param sequence: Tokenized sentences to train the network on.
        
        type word_cover: float
        param word_cover:  The percentage of words in sequence which should be included in the word list if it's generated by the model.  This parameter is ignored if the word list is passed in.
        """
        
        self.sentence_count = 0
        self.cos_similarity = None
        
        self.wrd_list = self.buildList(sequence, word_cover)            
        self.wrd_list.append(BEGIN_SEQ)
        self.wrd_list.append(END_SEQ)
        self.wrd_list.append(UNK)
    
        # create network
        c = cost(backend)
        a = activation(backend)
        self.embedding_dim = embedding_dim
        
        self.network = neural.NN(c.cross_entropy, c.dcross_entropy, backend_type = backend)
        self.network.addLayer(layer.Layer(len(self.wrd_list), embedding_dim, a.rectified, a.d_rectified, backend_type=backend))
        self.network.addLayer(layer.Layer(embedding_dim, 2*embedding_dim, a.rectified, a.d_rectified, backend_type=backend))
        self.network.addLayer(layer.Layer(2*embedding_dim, 2*embedding_dim, a.rectified, a.d_rectified, backend_type=backend))
        self.network.addLayer(layer.Layer(2*embedding_dim, embedding_dim, a.rectified, a.d_rectified, backend_type=backend))
        self.network.addLayer(layer.Layer(embedding_dim, len(self.wrd_list), a.softmax, a.d_softmax, backend_type=backend))
        
        # all parameters can be passed in later
        self.network.learn_rate = 0.05
        self.network.momentum = 0.5
        self.network.l2_coefficient = 1e-4
        #self.network.gradient_clip = 50
        
        self.backend = backend
        self.sentences = sequence
        self.forget_factor = forget_factor

    
    def buildList(self, sequence, word_cover):
        """
        Mainly an internal method to build a list of words from a sequence of sentences.
        
        type sequence: list of list
        param sequence:  Tokenized sentences to build wordl ist from.
        
        type word_cover: float
        param word_cover: Percentage of words to cover from the sentence by word count.
        
        rtype: list
        return: list of words which cover at least word_cover percentage of total words in the sample.
        """
        
        word_dictionary = defaultdict(int)
        all_words_count = 0.
        self.sentence_count = 0.
        for s in sequence:
            self.sentence_count += 1
            for w in s:
                word_dictionary[w] += 1
                all_words_count += 1
        
        # sort dictionary by word count in descending order
        ordered_word_list = sorted(word_dictionary.items(), key=operator.itemgetter(1), reverse=True)
        wrd_list = list()
        cover_count = 0.
        
        # keep adding words to the dictionary so long as the count of words covered as a proportion of word count is less than word_cover
        for word,count in ordered_word_list:
            if (cover_count + count) / (all_words_count) > word_cover:
                # adding this word to the final dictionary breaks over the threshold so don't include it and break out
                break
            cover_count += count
            wrd_list.append(word)
        return wrd_list
     
    def train(self, epochs=100, model_save_interval = -1, save_location = None, model_name='', update_type='vanilla', batch_size=128, l2_coefficient = 0):
        """
        Train network 
        """
        
        # create network trainer
        params = dict()
        params['epochs'] = 10
        params['updateType'] = update_type
        params['batchSize'] = batch_size
        nnt = trainer.NNTrainer(parameters = params)
        
        self.network.l2_coefficient = l2_coefficient
        
        # debug methods for the trainer
        def postUpdate(idx, dw, db, err, output, input_values, targets, network):
            if (idx % 16384) == 0:
                output_class = np.argmax(output, axis=1)
                target_class = np.argmax(targets, axis=1)
                correct = 0
                for o, t in zip(output_class, target_class):
                    if (o == t):
                        correct += 1
                
                p = np.power(2, np.average(err))
                print("(Training) Err: %1.4f\tCorrect: %0.2f%%" % (np.average(err), 100 * correct / len(targets)))
                print("(Training) Perplexity: %0.2f" % (p))
            pass

        def postEpoch(e, network):
            print "Epoch %s" % (e)
            network.diagnostic()
        
        nnt.setPostUpdate(postUpdate)
        nnt.setPostEpoch(postEpoch)
        
        # leave 10% for validaiton
        train_idx = int(self.sentence_count * 0.9)
        valid_idx = self.sentence_count - 1
        
        # 8e9 is for a quarter of available RAM which is reasonable given this is an approximation.  16 is for float64 and the other two parameters are the matrix size of each batch
        # memmult is the number of batches to accumulate before sending to the trainer.  This can be improved if we use float32 (double the size)
        memmult = int(3e9 / (params['batchSize'] * len(self.wrd_list) * 16))
        print memmult
        
        e = 0
        start_freeze = False
        prev_valid = 1.
        burn_in = 10
        prev_perplexity = 0.
        
        while e < epochs:
            correct_count = 0.
            valid_size = 0.
            sentence_count = 0
            inputs = list()
            targets = list()
            #self.network.resetRMS()
            e += 1
            print "\n============ Epoch %s ============" % (e)
            
            if model_save_interval <> -1:
                if (e <> 0) and (e % model_save_interval == 0):
                    print "Saving network to disk..."
                    self.save(save_location, model_name)                    
            
            for s in self.sentences:
                # collect samples before training
                s = [BEGIN_SEQ] + s
                s = [ w if w in self.wrd_list else UNK for w in s ]
                
                one_hot_sequence = [ fofe.one_hot(w, self.wrd_list) for w in s ]
                input_value = fofe.biFOFE(one_hot_sequence, alpha=self.forget_factor)
                #input_value = one_hot_sequence
                target_sentence = s[1:] + [ END_SEQ ]
                target_value = [ fofe.one_hot(w, self.wrd_list) for w in target_sentence ]        
                inputs.extend(input_value)
                targets.extend(target_value)
                
                if sentence_count <= train_idx:
                    # training mode
                    if (len(inputs) >= memmult*params['batchSize']) or (sentence_count == train_idx):
                        print "\nTraining batch (%s)" % (len(inputs))
                        train_x = np.matrix(inputs)
                        train_t = np.matrix(targets)
                        train_data = dataset.dataset(x=train_x, y=train_t, backend_type=self.backend)
                        print train_data.length
                        nnt.train(self.network, train_data)
                        
                        inputs = list()
                        targets = list()
                        train_x = []
                        train_t = []
                        train_data = None
                        
                elif sentence_count <= valid_idx:
                    # validation  mode   
                    # should compute perplexity per sentence
                    
                    if (len(inputs) >= memmult*params['batchSize']) or (sentence_count == valid_idx):
                        print '\nValidation batch (%s)' % (len(inputs))
                        valid_x = np.matrix(inputs)
                        valid_t = np.matrix(targets)
                        valid_data = dataset.dataset(x=valid_x, y=valid_t, backend_type=self.backend)
                        print valid_data.length
                        x, y = valid_data.getbatch(params['batchSize'])
                        total_err = 0.
                        while x is not None:
                            valid_size += len(x)
                            output = self.network.forward(x, train=False)
                            err = self.network.cost(output, y)
                            total_err += np.sum(err)  # accumulated for perplexity calculation
                            
                            x, y = valid_data.getbatch(params['batchSize'])
                        
                        inputs = list()
                        targets = list()
                        valid_x = []
                        valid_t = []
                        valid_data = None
                        
                sentence_count += 1
            
            valid_shown = False
            
            prev_valid = float(correct_count / valid_size)
            perplexity = np.power(2, total_err / valid_size)
            if (perplexity > prev_perplexity - 1) and not start_freeze:
                self.network.learn_rate /= 2
                epochs = e+6
                start_freeze = True
                print "start_freeze"
                
            prev_perplexity = perplexity
            print("(Validation) Correct: %0.2f%% (%s/%s)" % (100 * (correct_count / valid_size), correct_count, valid_size))
            print("(Validation) Perplexity: %0.2f (average cross-entropy: %s)" % (perplexity, (total_err / valid_size)))

    def next_word(self, sequence, mode='max'):
        """
        Generate the next word given a sequence of words.
        
        type sequence: list
        param sequence: sequence of starting words
        
        type mode: string
        param mode:  'max' means take maximum probability in the softmax, 'random' means select from softmax output using it's probabilities
        
        rtype: string
        return: next word generated
        """
        seq = [BEGIN_SEQ] + sequence
        seq = [ w if w in self.wrd_list else UNK for w in seq ]
        one_hot_sequence = [ fofe.one_hot(w, self.wrd_list) for w in seq ]
        input_value = fofe.biFOFE(one_hot_sequence, alpha=self.forget_factor)
        x_mat = self.network.back.array([input_value[-1]])
        output = self.network.forward(x_mat, train=False)
        if mode == 'max':
            # return the most likely word
            wrd_idx = np.argmax(output, axis=1)
            return self.wrd_list[wrd_idx]
        elif mode == 'random':
            # not working because numpy likes probs to sum to 1 and there's some rounding issue here...
            probs = np.array(output[0]).astype(np.float64)
            probs /= probs.sum()
            print repr(probs.sum())
            
            wrd_idx = np.random.choice(len(self.wrd_list), p=probs)
            return self.wrd_list[wrd_idx]
    
    def cosine_similarity(self, w):
        if self.cos_similarity == None:
            embedding_matrix = np.matrix(self.network.layers[0].W)
            # can be GPU'd
            e_norm = np.linalg.norm(embedding_matrix, axis=1)
            dot = np.dot(embedding_matrix, embedding_matrix.T)
            e_norm = np.matrix(e_norm)
            self.cos_similarity = dot/(e_norm.T * e_norm)
    
        if w not in self.wrd_list:
            return []
        wrd_idx = self.wrd_list.index(w)
        sims = np.array(self.cos_similarity[wrd_idx])[0]
        sim_wrds = [ (self.wrd_list[idx], sim) for idx, sim in enumerate(list(sims)) ]
        sim_wrds = sorted(sim_wrds, key=lambda x: x[1], reverse=True)
        return sim_wrds
    
    def save(self, location, model_name):
        # save model file
        self.network.pickle(os.path.join(location, model_name + "_net.pkl"))
        model_params = dict()
        model_params['embedding_dim'] = self.embedding_dim
        model_params['backend'] = self.backend
        model_params['forget_factor'] = self.forget_factor
        model_params['wrd_list'] = self.wrd_list
        model_out = open(os.path.join(location, model_name + "_model.pkl"), 'wb')
        pickle.dump(model_params, model_out)
        model_out.close()
        
    def load(self, location, model_name):
        self.network.load(os.path.join(location, model_name + "_net.pkl"))
        model_in = open(os.path.join(location, model_name + "_model.pkl"), 'rb')
        model_params = pickle.load(model_in)
        model_in.close()
        self.embedding_dim = model_params['embedding_dim']
        self.backend = model_params['backend']
        self.wrd_list = model_params['wrd_list']
        self.forget_factor = model_params['forget_factor']



